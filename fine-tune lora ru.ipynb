{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9807019",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets evaluate accelerate timm ipykernel ipywidgets IProgress scikit-learn scipy peft bitsandbytes trl streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed72fd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eab9d76df84e5c9ca495834b88ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49683fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this restricts libs to only only allow specified gpus to be used\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "CUDA_DEVICE_IDX = 5\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{CUDA_DEVICE_IDX}\"\n",
    "device_map = f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_map)\n",
    "\n",
    "def set_seed(random_seed: int = 42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef27fec",
   "metadata": {},
   "source": [
    "# Choosing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec69c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fa877",
   "metadata": {},
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f965f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this to download Dostaevskiy's text corpus\n",
    "# !wget https://gitlab.com/z00logist/artificial-dostoevsky/-/raw/main/data/corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a477ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text example: Ох уж эти мне сказочники! Нет чтобы написать что-нибудь полезное, приятное, усладительное, а то всю подноготную в земле вырывают!.. Вот уж запретил бы им писать! Ну, на что это похоже: читаешь... невольно задумаешься, а там всякая дребедень и пойдет в голову; право бы, запретил им писать; так-таки п\n",
      "Length of full corpus: 9652088\n"
     ]
    }
   ],
   "source": [
    "raw_text = open('corpus.txt').read()\n",
    "print('Text example:', raw_text[:300])\n",
    "print('Length of full corpus:', len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237e99a",
   "metadata": {},
   "source": [
    "# Let's make Train/Validation/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f117ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 965.2088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'start_idx'],\n",
       "        num_rows: 757\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'start_idx'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'start_idx'],\n",
       "        num_rows: 117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from datasets import Dataset, DatasetDict\n",
    "# let's divide text into chunks of roughly 10.000 chars,\n",
    "# then shuffle them between splits\n",
    "document_size = 10000\n",
    "print('Number of documents:', len(raw_text) / document_size)\n",
    "\n",
    "def get_split_idx(sizes: list | tuple, size: int):\n",
    "    \"\"\"Returns idx for split according to distributions of sizes\"\"\"\n",
    "    idx = torch.arange(0, size)\n",
    "    probabilities = torch.tensor(sizes)\n",
    "    probabilities /= probabilities.sum()\n",
    "    split_mask = torch.multinomial(probabilities, num_samples=size, replacement=True)\n",
    "    return tuple((idx[split_mask == i].numpy().tolist() for i in range(len(sizes))))\n",
    "\n",
    "def get_dataset_dict(text: str, sizes_dict: OrderedDict):\n",
    "    split_idx = get_split_idx(list(sizes_dict.values()), int(len(text) / document_size))\n",
    "    split_idx = {split: split_idx[i] for i, split in enumerate(sizes_dict)}\n",
    "    datasets = DatasetDict() \n",
    "    for split, idx in split_idx.items():\n",
    "        datasets[split] = Dataset.from_dict({'text': [text[i * document_size: i * document_size + document_size] for i in idx], 'start_idx': [i * document_size for i in idx]})\n",
    "    return datasets\n",
    "\n",
    "set_seed(52)\n",
    "datasets = get_dataset_dict(raw_text, OrderedDict([('train', 0.8), ('validation', 0.1), ('test', 0.1)]))\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51967763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а! Цена всех одиннадцати книг, присовокупив сюда издержки на переплет, была по крайней мере рублей шестьдесят. Где взять денег? Я думала-думала и не знала, на что решиться. У матушки просить не хотелось. Конечно, матушка мне непременно бы помогла; но тогда все бы в доме узнали о нашем подарке; да к тому же этот подарок обратился бы в благодарность, в плату за целый год трудов Покровского. Мне хотелось подарить одной, тихонько от всех. А за труды его со мною я хотела быть ему навсегда одолженною без какой бы то ни было уплаты, кроме дружбы моей. Наконец я выдумала, как выйти из затруднения. Я знала, что у букинистов в Гостином дворе можно купить книгу иногда в полцены дешевле, если только поторговаться, часто малоподержанную и почти совершенно новую. Я положила непременно отправиться в Гостиный двор. Так и случилось; назавтра же встретилась какая-то надобность и у нас и у Анны Федоровны. Матушке понездоровилось, Анна Федоровна очень кстати поленилась, так что пришлось все поручения возложить на меня, и я отправилась вместе с Матреной. К моему счастию, я нашла весьма скоро Пушкина, и в весьма красивом переплете. Я начала торговаться. Сначала запросили дороже, чем в лавках; но потом, впрочем не без труда, уходя несколько раз, я довела купца до того, что он сбавил цену и ограничил свои требования только десятью рублями серебром. Как мне весело было торговаться!.. Бедная Матрена не понимала, что со мной делается и зачем я вздумала покупать столько книг. Но ужас! Весь мой капитал был в тридцать рублей ассигнациями, а купец никак не соглашался уступить дешевле. Наконец я начала упрашивать, просила-просила его, наконец упросила. Он уступил, но только два с полтиною, и побожился, что и эту уступку он только ради меня делает, что я такая барышня хорошая, а что для другого кого он ни за что бы не уступил. Двух с половиною рублей недоставало! Я готова была заплакать с досады. Но самое неожиданное обстоятельство помогло мне в моем горе. Недалеко от меня, у другого стола с книгами, я увидала старика Покровского. Вокруг него столпились четверо или пятеро букинистов; они его сбили с последнего толку, затормошили совсем. Всякий из них предлагал ему свой товар, и чего-чего не предлагали они ему, и чего-чего не хотел он купить! Бедный старик стоял посреди их, как будто забитый какой-нибудь, и не знал, за что взяться из того, что ему предлагали. Я подошла к нему и спросила  что он здесь делает? Старик мне очень обрадовался; он любил меня без памяти, может быть, не менее Петеньки. \"Да вот книжки покупаю, Варвара Алексеевна, отвечал он мне, Петеньке покупаю книжки. Вот его день рождения скоро будет, а он любит книжки, так вот я и покупаю их для него...\" Старик и всегда смешно изъяснялся, а теперь вдобавок был в ужаснейшем замешательстве. К чему ни приценится, всё рубль серебром, два рубля, три рубля серебром; уж он к большим книгам и не приценивался, а так только завистливо на них посматривал, перебирал пальцами листочки, вертел в руках и опять их ставил на место. \"Нет, нет, это дорого,-говорил он вполголоса, а вот разве отсюдова что-нибудь\", и тут он начинал перебирать тоненькие тетрадки, песенники, альманахи; это всё было очень дешево. \"Да зачем вы это всё покупаете, спросила я его, это всё ужасные пустяки\". \"Ах, нет, отвечал он, нет, вы посмотрите только, какие здесь есть хорошие книжки; очень, очень хорошие есть книжки!\" И последние слова он так жалобно протянул нараспев, что мне показалось, что он заплакать готов от досады, зачем книжки хорошие дороги, и что вот сейчас капнет слезинка с его бледных щек на красный нос. Я спросила, много ли у него денег? \"Да вот, тут бедненький вынул все свои деньги, завернутые в засаленную газетную бумажку, вот полтинничек, двугривенничек, меди копеек двадцать\". Я его тотчас потащила к моему букинисту. \"Вот целых одиннадцать книг стоит всего-то тридцать два рубля с полтиною; у меня есть тридцать; приложите два с полтиною, и мы купим все эти книги и подарим вместе\". Старик обезумел от радости, высыпал все свои деньги, и букинист навьючил на него всю нашу общую библиотеку. Мой старичок наложил книг во все карманы, набрал в обе руки, под мышки и унес всё к себе, дав мне слово принести все книги на другой день тихонько ко мне. На другой день старик пришел к сыну, с часочек посидел у него по обыкновению, потом зашел к нам и подсел ко мне с прекомическим таинственным видом. Сначала с улыбкой, потирая руки от гордого удовольствия владеть какой-нибудь тайной, он объявил мне, что книжки все пренезаметно перенесены к нам и стоят в уголку, в кухне, под покровительством Матрены. Потом разговор естественно перешел на ожидаемый праздник; потом старик распространился о том, как мы будем дарить, и чем далее углублялся он в свой предмет, чем более о нем говорил, тем приметнее мне становилось, что у него есть что-то на душе, о чем он не может, не смеет, даже боится выразиться. Я всё ждала и молчала. Тайная радость, тайное удовольствие, что я легко читала доселе в его странных ухватках, гримасничанье, подмигиванье левым глазком, исчезли. Он делался поминутно всё беспокойнее и тоскливее; наконец он не выдержал.  Послушайте, начал он робко, вполголоса, послушайте, Варвара Алексеевна... знаете ли что, Варвара Алексеевна?.. Старик был в ужасном замешательстве. Видите: вы, как придет день его рождения, возьмите десять книжек и подарите их ему сами, то есть от себя, с свое! стороны; я же возьму тогда одну одиннадцатую и уж тоже подарю от себя, то есть собственно с своей стороны. Так вот, видите ли  и у вас будет что-нибудь подарить, и у меня будет что-нибудь подарить; у нас обоих будет что-нибудь подарить. Тут старик смешался и замолчал. Я взглянула на него; он с робким ожиданием ожидал моего приговора. \"Да зачем же вы хотите, чтоб мы не вместе дарили, Захар Петрович?\"  \"Да так, Варвара Алексеевна, уж это так... я ведь, оно, того...\"  одним словом, старик замешался, покраснел, завяз в своей фразе и не мог сдвинуться с места.  Видите ли, объяснился он наконец. Я, Варвара Алексеевна, балуюсь подчас... то есть я хочу доложить вам, что я почти и всё балуюсь и всегда балуюсь... придерживаюсь того, что нехорошо... то есть, знаете, этак на дворе такие холода бывают, также иногда неприятности бывают разные, или там как-нибудь грустно сделается, или что-нибудь из нехорошего случится, так я и не удержусь подчас, и забалуюсь, и выпью иногда лишнее. Петруше это очень неприятно. Он вот, видите ли, Варвара Алексеевна, сердится, бранит меня и мне морали разные читает. Так вот бы мне и хотелось теперь самому доказать ему подарком моим, что я исправляюсь и начинаю вести себя хорошо. Что вот я копил, чтобы книжку купить, долго копил, потому что у меня и денег-то почти никогда не бывает, разве, случится, Петруша кое-когда даст. Он это знает. Следовательно, вот он увидит употребление денег моих и узнает, что всё это я для него одного делаю. Мне стало ужасно жаль старика. Я думала недолго. Старик смотрел на меня с беспокойством. \"Да слушайте, Захар Петрович, сказала я, вы подарите их ему все!\"  \"Как все? то есть книжки все?..\"  \"Ну да, книжки все\". \"И от себя?\"  \"От себя\". \"От одного себя? то есть от своего имени?\"  \"Ну да, от своего имени...\" Я, кажется, очень ясно толковала, но старик очень долго не мог понять меня. \"Ну да, говорил он, задумавшись, да! это будет очень хорошо, это было бы весьма хорошо, только вы-то как же, Варвара Алексеевна?\"  \"Ну, да я ничего не подарю\". \"Как!  закричал старик, почти испугавшись, так вы ничего Петеньке не подарите, так вы ему ничего дарить не хотите?\" Старик испугался; в эту минуту он, кажется, готов был отказаться от своего предложения затем, чтобы и я могла чем-нибудь подарить его сына. Добряк был этот старик! Я уверила его, что я бы рада была подарить что-нибудь, да только у него не хочу отнимать удовольствия. \"Если сын ваш будет доволен, прибавила я, и вы будете рады, то и я буду рада, потому что втайне-то, в сердце-то моем, буду чувствовать, как будто и на самом деле я подарила\". Этим старик совершенно успокоился. Он пробыл у нас еще два часа, но всё это время на месте не мог усидеть, вставал, возился, шумел, шалил с Сашей, целовал меня украдкой, щипал меня за руку и делал тихонько гримасы Анне Федоровне. Анна Федоровна прогнала его наконец из дома. Одним словом, старик от восторга так расходился, как, может быть, никогда еще не бывало с ним. В торжественный день он явился ровно в одиннадцать часов, прямо от обедни, во фраке, прилично заштопанном, и действительно в новом жилете и в новых сапогах. В обеих руках было у него по связке книг. Мы все сидели тогда в зале у Анны Федоровны и пили кофе (было воскресенье). Старик начал, кажется, с того, что Пушкин был весьма хороший стихотворец; потом, сбиваясь и мешаясь, перешел вдруг на то, что нужно вести себя хорошо и что если человек не ведет себя хорошо, то значит, что он балуется; что дурные наклонности губят и уничтожают человека; исчислил даже несколько пагубных примеров невоздержания и заключил тем, что он с некоторого времени совершенно исправился и что теперь ведет себя примерно хорошо. Что он и прежде чувствовал справедливость сыновних наставлений, что он всё это давно чувствовал и всё на сердце слагал, но теперь и на деле стал удерживаться. В доказательство чего дарит книги на скопленные им, в продолжение долгого времени, деньги. Я не могла удержаться от слез и смеха, слушая бедного старика; ведь умел же налгать, когда нужда пришла! Книги были перенесены в комнату Покровского и поставлены на полку. Покровский тотчас угадал истину. Старика пригласили обедать. Этот день мы все были так веселы. После обеда играли в фанты, в карты; Саша резвилась, я от нее не отставала. Покровский был ко мне внимателен и всё искал случая поговорить со мною наедине, но я не давалась. Это был лучший день в целые четыре года моей жизни. А теперь всё пойдут грустные, тяжелые воспоминания; начнется повесть о моих черных днях. Вот отчего, может быть, перо мое начинает двигаться медленнее и как будто отказывается писать далее. Вот отчего, может быть, я с таким увлечением\n",
      "start_idx: [70000, 90000, 100000], chunk_sizes: [10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "# let's check (chunk_sizes are not all the same though)\n",
    "print(datasets['train'][7]['text'])\n",
    "print(f\"start_idx: {datasets['train'][7:10]['start_idx']}, chunk_sizes: {list(map(len, datasets['train'][7:10]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0b5d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da3acddb1bc45a287d66582d9bc0b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2617 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7f5a24b319442fa2f094c2c23b9fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744bd63c18c4263a8773ed5f261eea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 757\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing before chunking\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "tokenized_datasets = datasets.map(tokenize_function, remove_columns=['start_idx', 'text'], batched=True).remove_columns('attention_mask')\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4167fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_sizes: [2617, 2566, 2578, 2529, 2380, 2325, 2352, 2376, 2598, 2631]\n"
     ]
    }
   ],
   "source": [
    "# let's see what we get\n",
    "print(f\"document_sizes: {list(map(len, tokenized_datasets['train'][0:10]['input_ids']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf52baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c80c3a74e584f0da5474a78712a6575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        num_shards: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's take these documents and chunk them up each, chunks will be of size 1024. In order to fit in RAM restrictions, we'll use IterableDataset and use generator\n",
    "from datasets import IterableDataset, IterableDatasetDict\n",
    "from random import randint\n",
    "from functools import partial\n",
    "chunk_size = 1024\n",
    "\n",
    "def chunk_generator(dataset: Dataset, sequential: bool = False):\n",
    "    \"\"\"Generator that (maybe randomly) picks documents from the dataset and then random chunk of size chunk_size\"\"\"\n",
    "    if sequential:\n",
    "        for document_idx in range(len(dataset)):\n",
    "            tokens = dataset[document_idx]['input_ids']\n",
    "            i = randint(0, len(tokens) - chunk_size)\n",
    "            yield {'input_ids': tokens[i: i + chunk_size]}\n",
    "    else:\n",
    "        while True:\n",
    "            document_idx = randint(0, len(dataset) - 1)\n",
    "            tokens = dataset[document_idx]['input_ids']\n",
    "            i = randint(0, len(tokens) - chunk_size)\n",
    "            yield {'input_ids': tokens[i: i + chunk_size]}\n",
    "\n",
    "set_seed(2718)\n",
    "# it's iterable, so it's light-weight\n",
    "chunked_data = IterableDatasetDict()\n",
    "for split in tokenized_datasets:\n",
    "    dataset = tokenized_datasets[split]\n",
    "    chunked_data[split] = (IterableDataset if split == 'train' else Dataset).from_generator(\n",
    "        partial(chunk_generator, dataset=dataset, sequential=split != 'train')\n",
    "    )\n",
    "tokenized_datasets = chunked_data\n",
    "chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4752ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 1260.62 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8df26f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check dtype of the dataset\n",
    "type(list(tokenized_datasets['train'].take(1))[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffbc4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) # does job for creating attention_mask and labels for us (it just copies inputs for labels, bcause shifting happens inside the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f908e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   16,  1012,   784,  ...,   669,  4223,   309],\n",
       "        [13134,  2625,   519,  ...,  1441,    16,   687],\n",
       "        [  299,  1186,  1248,  ...,   433,   968,    18],\n",
       "        [   16,   670,    16,  ...,  1335, 35338,    18],\n",
       "        [ 2754,  5804,   289,  ...,   503,  1042,    18]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[   16,  1012,   784,  ...,   669,  4223,   309],\n",
       "        [13134,  2625,   519,  ...,  1441,    16,   687],\n",
       "        [  299,  1186,  1248,  ...,   433,   968,    18],\n",
       "        [   16,   670,    16,  ...,  1335, 35338,    18],\n",
       "        [ 2754,  5804,   289,  ...,   503,  1042,    18]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = data_collator(list(tokenized_datasets[\"train\"].take(5)))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5eb39",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4bc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ab98c",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eddbf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, # 8-256\n",
    "    lora_alpha=32, # (>= r) alpha/r ~ 2-4\n",
    "    target_modules=[\"c_attn\", \"c_proj\"], \n",
    "    lora_dropout=0.05, # 0.01-0.2\n",
    "    # bias=\"none\", # train only LoRA weights, no biases \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6099201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,488,064 || all params: 766,788,096 || trainable%: 0.8461\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11239ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce668ad59d5e4e96a8da42c587760131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./rugpt3large_lora_results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.5,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./rugpt3large_lora_logs\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    "    # packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b684d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 13:30, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.314500</td>\n",
       "      <td>3.280031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.320500</td>\n",
       "      <td>3.257753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.278800</td>\n",
       "      <td>3.243949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.275000</td>\n",
       "      <td>3.235051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.295000</td>\n",
       "      <td>3.229133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.256300</td>\n",
       "      <td>3.225114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.283900</td>\n",
       "      <td>3.222836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.250100</td>\n",
       "      <td>3.221640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.264000</td>\n",
       "      <td>3.221438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.244200</td>\n",
       "      <td>3.220997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=3.277965326309204, metrics={'train_runtime': 825.3778, 'train_samples_per_second': 1.939, 'train_steps_per_second': 0.061, 'total_flos': 6748054644326400.0, 'train_loss': 3.277965326309204})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(11)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c871431",
   "metadata": {},
   "source": [
    "# Let's test it out (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696898b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "checkpoint = 'checkpoint-50'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int = 40, temperature: float = 0.7, top_p: float = 0.9, seed: int | float | None = 52, **kwargs):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    set_seed(seed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,\n",
    "            **kwargs\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_text_beam(prompt: str, \n",
    "                       max_length: int = 40, \n",
    "                       num_beams: int = 10, \n",
    "                       num_return_sequences: int = 3, \n",
    "                       no_repeat_ngram_size: int = 2, \n",
    "                       num_beam_groups: int = 2,\n",
    "                       diversity_penalty: float = 1.0,\n",
    "                       seed: int | float | None = 52,\n",
    "                       **kwargs):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    set_seed(seed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequences = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams, # beam width \n",
    "            early_stopping=True, # stop when all beams finish\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size, # n-gram penalties, technique to avoid repetition\n",
    "            num_return_sequences=num_return_sequences, # return top k sequences\n",
    "            num_beam_groups=num_beam_groups, # divides num_beams to this number groups. two groups differ from each other. greatly combines with diversity_penalty \n",
    "            diversity_penalty=diversity_penalty, # encourages diversity beams (if using num_beam_groups)\n",
    "            trust_remote_code=True, # beam search backward compatibility\n",
    "            **kwargs\n",
    "        )\n",
    "    return tokenizer.batch_decode(sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e41acea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_text_beam(\"Быть или не быть?\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=1, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.0,\n",
    "                       seed=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5cbe0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Быть или не быть? Вот в чем вопрос.\n",
      "\n",
      "— Да, да, конечно, — сказал я, и мы оба засмеялись. Потом я спросил: — А как вы думаете, что будет с нами, если мы все-таки останемся в живых? Что будет, когда мы умрем? Как мы будем жить после смерти? Ведь мы же не знаем, как жить. Мы даже не можем сказать, кто мы такие, откуда мы, куда мы идем, зачем мы\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бог ему судья!\n",
      "\n",
      "— Да, да, я знаю, — сказал я, и мы оба засмеялись. Потом, помолчав немного, прибавил: — Я, право, не понимаю, как это могло случиться, что вы так легко поверили этому человеку. Ведь он, по-видимому, сумасшедший. Я не могу себе представить, чтобы вы могли поверить ему, если бы он не был сумасшедшим. Но ведь это не так, правда? Ведь вы не могли бы ему поверить,\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Бог ему судья! — крикнул кто-то из толпы.\n",
      "\n",
      "— А ты кто такой будешь, чтобы судить его? Ты, что ли? А ну-ка скажи, кто ты такой есть, и я тебе покажу, как ты судишь его! Я тебе сейчас покажу! Ты мне сейчас покажешь, где у него деньги лежат! Вот тебе деньги! Видишь? Вот они, вот они! А ты говоришь: «Не знаю, не знаю!» А вот тебе еще деньги,\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Бог ему судья! Но, может быть, он и в самом деле не знает, что делает?\n",
      "\n",
      "— Не знаю. Но если бы он знал, то не стал бы делать этого. Он не может не знать. И я не знаю, почему он не сделал этого раньше. Я не могу понять, как он мог не сделать этого теперь, когда все уже кончено и ничего уже нельзя поправить. Ведь он же мог бы сделать это раньше, если б только захотел. А теперь уже ничего нельзя\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Бог ему судья!\", \n",
    "                       max_length=100, \n",
    "                       num_beams=15, \n",
    "                       num_return_sequences=15, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=3,\n",
    "                       diversity_penalty=1.0,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[5], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И все-таки, тварь ли я дрожащая или право имею?\n",
      "\n",
      "— Право имеете, ваше сиятельство, право имеете! — с жаром подтвердил он. Да ведь я и сам не знаю, что со мной делается! Я, кажется, с ума сойду, если с вами не поговорю. Вы, может быть, сердитесь на меня, а я не сержусь на вас, потому что я вас уважаю, и вы меня уважаете. Я вас\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "И все-таки, тварь ли я дрожащая или право имею? И что мне делать?\n",
      "\n",
      "— Что делать, что делать! — передразнил он ее с раздражением и даже с досадой.    Право имеете, право имеете!  повторил он с каким-то торжеством, как будто и в самом деле было что-нибудь в этом роде.  Что же вы хотите сделать?  спросила она с беспокойством. Он не отвечал, и она повторила\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"И все-таки, тварь ли я дрожащая или право имею?\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e76afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какое лучшее определение человека?\n",
      "Человек - это то, что он о себе думает.\n",
      "Что делать, если я не хочу идти в школу, а родители не хотят идти со мной? Что мне делать? (((\n",
      "Скажи родителям, чтобы они тебя отвели в другую школу. А если они не согласятся, то скажи что-нибудь обидное, например: \"Не хочу я в эту школу! \" и т. д. Если родители будут настаивать на своём, скажи им,\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Какое лучшее определение человека?\n",
      "\"Человек - это звучит гордо!\"\n",
      "Что делать, если я не знаю, что мне делать? Я не могу понять, как мне жить дальше. Помогите, пожалуйста, советом.\n",
      "Учиться, учиться и еще раз учиться! А то так и будете всю жизнь сидеть на шее у родителей!\n",
      "Я и так сижу на маминой шее. Я уже не говорю о том, чтобы учиться. Учиться я хочу, но как-то не получается.\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Какое лучшее определение человека?\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39b5daff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сомневаюсь на правильном ли я пути...\n",
      "Что делать, если я не знаю, куда идти?\n",
      "Идти туда, где не знают, что делать.\n",
      "А если не знаешь, как идти, то и не надо никуда идти. Зачем тогда вообще что-то делать? Зачем вообще жить? Я не хочу жить, я хочу умереть. Я хочу, чтобы меня оставили в покое, и чтобы никто меня не трогал. Мне не нужно ничего, кроме тишины и покоя.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Сомневаюсь на правильном ли я пути... (с)\n",
      "Что делать, если не знаешь, куда идти?\n",
      "Идти туда, не знаю куда, но точно знаю, что не в ту сторону.\n",
      "Добрый вечер, Владимир! Спасибо за Ваш ответ! Я тоже так думаю. Но всё-таки не могу понять, как можно не знать, в какую сторону идти, и куда именно идти. Ведь это же так просто! И в то же время это очень сложно!\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Сомневаюсь на правильном ли я пути...\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51805e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Русский весьма часто смеется там, где говорит по-французски, а по-итальянски не смеётся никогда, потому что не понимает\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Русский весьма часто смеется там, где\", \n",
    "                       max_length=28, \n",
    "                       num_beams=15, \n",
    "                       num_return_sequences=15, \n",
    "                       no_repeat_ngram_size=3, \n",
    "                       num_beam_groups=3,\n",
    "                       diversity_penalty=1.0,\n",
    "                       seed=52)\n",
    "print(outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "78016180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жизнь задыхается без воздуха.\n",
      "\n",
      "— Да, да, я знаю, — сказал он, и глаза его заблестели, как у ребенка, когда он что-нибудь понимает. Он был в восторге от того, что его догадка подтвердилась. Но он не знал, радоваться ли ему или огорчаться, потому что, несмотря на все его старания, он все-таки не мог понять, в чем тут дело. И вдруг он вспомнил, где он и что с ним\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Жизнь задыхается без еды и воды.\n",
      "\n",
      "— Что же вы предлагаете? — спросил я, не понимая, к чему он клонит, и не зная, как ему ответить на его вопрос. Я не знал, что мне делать и что делать с ним. Мне было страшно и стыдно, но я ничего не мог с собой поделать. Он смотрел на меня так, словно хотел что-то сказать мне, а я не понимал, о чем он хочет меня спросить. Наконец, я\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Жизнь задыхается без\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d9a1cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Главное, самому себе не лгите... ибо лгущий самому себе теряет способность различать добро и зло, правду и ложь, истину и кривду, добро от зла, и наоборот. Ложь, как и правда, не нуждается в доказательствах, она сама по себе есть доказательство. Истина же, напротив, требует доказательств, ибо без них нет и не может быть истины. Если же вы хотите, чтобы кто-нибудь доказал вам истину, то докажите сначала, что он лгун\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Главное, самому себе не лгите... ибо лгущий самому себе теряет способность отличать добро от зла, правду от лжи, любовь от ненависти и дружбу от предательства.\n",
      "Что делать, если я не умею врать?\n",
      "Врать не надо, надо уметь говорить правду, и тогда ложь будет не нужна. А врать не нужно, потому что ложь - это тоже правда, но не та, которую ты хочешь услышать, а та которая есть на самом деле. Если ты будешь врать,\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Главное, самому себе не лгите... ибо лгущий самому себе теряет способность\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc0a31",
   "metadata": {},
   "source": [
    "## Some conclusions after experimenting\n",
    "- LoRA algorithm is amazing for fine-tuning large and medium size llms \\\n",
    "it provides a way to train even when other common memory saving techniques couldn't help out\n",
    "- adafactor optimizer (though it wasn't used eventually) is a fantastic replacement for AdamW\n",
    "- gradient_accumulation idea is a real saver for imitating large batch size\n",
    "- quantinization and fp16 especially are musthave for nlp tasks\n",
    "- iterable, streaming datasets combining with generators are light-weight and really handy   \n",
    "- DataCollatorForLanguageModeling, return_overflowing_tokens, return_overflowing_tokens and other stuff from hf libraries rid of so much boilerplate code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10 Poetry",
   "language": "python",
   "name": "poetry_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
