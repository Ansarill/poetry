{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9807019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers datasets evaluate accelerate timm ipykernel ipywidgets IProgress scikit-learn scipy peft bitsandbytes trl streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed72fd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f62c3e21df4593b025534edee92a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49683fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this restricts libs to only only allow specified gpus to be used\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "CUDA_DEVICE_IDX = 5\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{CUDA_DEVICE_IDX}\"\n",
    "device_map = f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_map)\n",
    "\n",
    "def set_seed(random_seed: int = 42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef27fec",
   "metadata": {},
   "source": [
    "# Choosing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec69c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fa877",
   "metadata": {},
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f965f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this to download Dostaevskiy's text corpus\n",
    "# !wget https://gitlab.com/z00logist/artificial-dostoevsky/-/raw/main/data/corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a477ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text example: Ох уж эти мне сказочники! Нет чтобы написать что-нибудь полезное, приятное, усладительное, а то всю подноготную в земле вырывают!.. Вот уж запретил бы им писать! Ну, на что это похоже: читаешь... невольно задумаешься, а там всякая дребедень и пойдет в голову; право бы, запретил им писать; так-таки п\n",
      "Length of full corpus: 9652088\n"
     ]
    }
   ],
   "source": [
    "raw_text = open('corpus.txt').read()\n",
    "print('Text example:', raw_text[:300])\n",
    "print('Length of full corpus:', len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237e99a",
   "metadata": {},
   "source": [
    "# Let's make Train/Validation/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f117ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 965.2088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'start_idx'],\n",
       "        num_rows: 757\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'start_idx'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'start_idx'],\n",
       "        num_rows: 117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from datasets import Dataset, DatasetDict\n",
    "# let's divide text into chunks of roughly 10.000 chars,\n",
    "# then shuffle them between splits\n",
    "document_size = 10000\n",
    "print('Number of documents:', len(raw_text) / document_size)\n",
    "\n",
    "def get_split_idx(sizes: list | tuple, size: int):\n",
    "    \"\"\"Returns idx for split according to distributions of sizes\"\"\"\n",
    "    idx = torch.arange(0, size)\n",
    "    probabilities = torch.tensor(sizes)\n",
    "    probabilities /= probabilities.sum()\n",
    "    split_mask = torch.multinomial(probabilities, num_samples=size, replacement=True)\n",
    "    return tuple((idx[split_mask == i].numpy().tolist() for i in range(len(sizes))))\n",
    "\n",
    "def get_dataset_dict_old(text: str, sizes_dict: OrderedDict):\n",
    "    \"\"\"Returns the splitted dataset according to sizes_dict (old code for reproducibility)\"\"\"\n",
    "    split_idx = get_split_idx(list(sizes_dict.values()), int(len(text) / document_size))\n",
    "    split_idx = {split: split_idx[i] for i, split in enumerate(sizes_dict)}\n",
    "    datasets = DatasetDict() \n",
    "    for split, idx in split_idx.items():\n",
    "        datasets[split] = Dataset.from_dict({'text': [text[i * document_size: i * document_size + document_size] for i in idx], 'start_idx': [i * document_size for i in idx]})\n",
    "    return datasets\n",
    "\n",
    "def get_dataset_dict_updated(text: str, train_size=0.8, seed=52):\n",
    "    \"\"\"Returns the splitted dataset according to train_size\"\"\"\n",
    "    datasets = Dataset.from_dict({'text': [text[i: i + document_size] for i in range(0, len(text), document_size)], \n",
    "                                           'start_idx': [i for i in range(0, len(text), document_size)]})\n",
    "    datasets = datasets.train_test_split(train_size=train_size, seed=seed)\n",
    "    val_test_datasets = datasets.pop('test').train_test_split(train_size=0.5, seed=seed)\n",
    "    datasets['validation'] = val_test_datasets.pop('train')\n",
    "    datasets['test'] = val_test_datasets.pop('test')\n",
    "    return datasets\n",
    "\n",
    "set_seed(52)\n",
    "datasets = get_dataset_dict_old(raw_text, OrderedDict([('train', 0.8), ('validation', 0.1), ('test', 0.1)]))\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51967763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а! Цена всех одиннадцати книг, присовокупив сюда издержки на переплет, была по крайней мере рублей шестьдесят. Где взять денег? Я думала-думала и не знала, на что решиться. У матушки просить не хотелось. Конечно, матушка мне непременно бы помогла; но тогда все бы в доме узнали о нашем подарке; да к тому же этот подарок обратился бы в благодарность, в плату за целый год трудов Покровского. Мне хотелось подарить одной, тихонько от всех. А за труды его со мною я хотела быть ему навсегда одолженною без какой бы то ни было уплаты, кроме дружбы моей. Наконец я выдумала, как выйти из затруднения. Я знала, что у букинистов в Гостином дворе можно купить книгу иногда в полцены дешевле, если только поторговаться, часто малоподержанную и почти совершенно новую. Я положила непременно отправиться в Гостиный двор. Так и случилось; назавтра же встретилась какая-то надобность и у нас и у Анны Федоровны. Матушке понездоровилось, Анна Федоровна очень кстати поленилась, так что пришлось все поручения возложить на меня, и я отправилась вместе с Матреной. К моему счастию, я нашла весьма скоро Пушкина, и в весьма красивом переплете. Я начала торговаться. Сначала запросили дороже, чем в лавках; но потом, впрочем не без труда, уходя несколько раз, я довела купца до того, что он сбавил цену и ограничил свои требования только десятью рублями серебром. Как мне весело было торговаться!.. Бедная Матрена не понимала, что со мной делается и зачем я вздумала покупать столько книг. Но ужас! Весь мой капитал был в тридцать рублей ассигнациями, а купец никак не соглашался уступить дешевле. Наконец я начала упрашивать, просила-просила его, наконец упросила. Он уступил, но только два с полтиною, и побожился, что и эту уступку он только ради меня делает, что я такая барышня хорошая, а что для другого кого он ни за что бы не уступил. Двух с половиною рублей недоставало! Я готова была заплакать с досады. Но самое неожиданное обстоятельство помогло мне в моем горе. Недалеко от меня, у другого стола с книгами, я увидала старика Покровского. Вокруг него столпились четверо или пятеро букинистов; они его сбили с последнего толку, затормошили совсем. Всякий из них предлагал ему свой товар, и чего-чего не предлагали они ему, и чего-чего не хотел он купить! Бедный старик стоял посреди их, как будто забитый какой-нибудь, и не знал, за что взяться из того, что ему предлагали. Я подошла к нему и спросила  что он здесь делает? Старик мне очень обрадовался; он любил меня без памяти, может быть, не менее Петеньки. \"Да вот книжки покупаю, Варвара Алексеевна, отвечал он мне, Петеньке покупаю книжки. Вот его день рождения скоро будет, а он любит книжки, так вот я и покупаю их для него...\" Старик и всегда смешно изъяснялся, а теперь вдобавок был в ужаснейшем замешательстве. К чему ни приценится, всё рубль серебром, два рубля, три рубля серебром; уж он к большим книгам и не приценивался, а так только завистливо на них посматривал, перебирал пальцами листочки, вертел в руках и опять их ставил на место. \"Нет, нет, это дорого,-говорил он вполголоса, а вот разве отсюдова что-нибудь\", и тут он начинал перебирать тоненькие тетрадки, песенники, альманахи; это всё было очень дешево. \"Да зачем вы это всё покупаете, спросила я его, это всё ужасные пустяки\". \"Ах, нет, отвечал он, нет, вы посмотрите только, какие здесь есть хорошие книжки; очень, очень хорошие есть книжки!\" И последние слова он так жалобно протянул нараспев, что мне показалось, что он заплакать готов от досады, зачем книжки хорошие дороги, и что вот сейчас капнет слезинка с его бледных щек на красный нос. Я спросила, много ли у него денег? \"Да вот, тут бедненький вынул все свои деньги, завернутые в засаленную газетную бумажку, вот полтинничек, двугривенничек, меди копеек двадцать\". Я его тотчас потащила к моему букинисту. \"Вот целых одиннадцать книг стоит всего-то тридцать два рубля с полтиною; у меня есть тридцать; приложите два с полтиною, и мы купим все эти книги и подарим вместе\". Старик обезумел от радости, высыпал все свои деньги, и букинист навьючил на него всю нашу общую библиотеку. Мой старичок наложил книг во все карманы, набрал в обе руки, под мышки и унес всё к себе, дав мне слово принести все книги на другой день тихонько ко мне. На другой день старик пришел к сыну, с часочек посидел у него по обыкновению, потом зашел к нам и подсел ко мне с прекомическим таинственным видом. Сначала с улыбкой, потирая руки от гордого удовольствия владеть какой-нибудь тайной, он объявил мне, что книжки все пренезаметно перенесены к нам и стоят в уголку, в кухне, под покровительством Матрены. Потом разговор естественно перешел на ожидаемый праздник; потом старик распространился о том, как мы будем дарить, и чем далее углублялся он в свой предмет, чем более о нем говорил, тем приметнее мне становилось, что у него есть что-то на душе, о чем он не может, не смеет, даже боится выразиться. Я всё ждала и молчала. Тайная радость, тайное удовольствие, что я легко читала доселе в его странных ухватках, гримасничанье, подмигиванье левым глазком, исчезли. Он делался поминутно всё беспокойнее и тоскливее; наконец он не выдержал.  Послушайте, начал он робко, вполголоса, послушайте, Варвара Алексеевна... знаете ли что, Варвара Алексеевна?.. Старик был в ужасном замешательстве. Видите: вы, как придет день его рождения, возьмите десять книжек и подарите их ему сами, то есть от себя, с свое! стороны; я же возьму тогда одну одиннадцатую и уж тоже подарю от себя, то есть собственно с своей стороны. Так вот, видите ли  и у вас будет что-нибудь подарить, и у меня будет что-нибудь подарить; у нас обоих будет что-нибудь подарить. Тут старик смешался и замолчал. Я взглянула на него; он с робким ожиданием ожидал моего приговора. \"Да зачем же вы хотите, чтоб мы не вместе дарили, Захар Петрович?\"  \"Да так, Варвара Алексеевна, уж это так... я ведь, оно, того...\"  одним словом, старик замешался, покраснел, завяз в своей фразе и не мог сдвинуться с места.  Видите ли, объяснился он наконец. Я, Варвара Алексеевна, балуюсь подчас... то есть я хочу доложить вам, что я почти и всё балуюсь и всегда балуюсь... придерживаюсь того, что нехорошо... то есть, знаете, этак на дворе такие холода бывают, также иногда неприятности бывают разные, или там как-нибудь грустно сделается, или что-нибудь из нехорошего случится, так я и не удержусь подчас, и забалуюсь, и выпью иногда лишнее. Петруше это очень неприятно. Он вот, видите ли, Варвара Алексеевна, сердится, бранит меня и мне морали разные читает. Так вот бы мне и хотелось теперь самому доказать ему подарком моим, что я исправляюсь и начинаю вести себя хорошо. Что вот я копил, чтобы книжку купить, долго копил, потому что у меня и денег-то почти никогда не бывает, разве, случится, Петруша кое-когда даст. Он это знает. Следовательно, вот он увидит употребление денег моих и узнает, что всё это я для него одного делаю. Мне стало ужасно жаль старика. Я думала недолго. Старик смотрел на меня с беспокойством. \"Да слушайте, Захар Петрович, сказала я, вы подарите их ему все!\"  \"Как все? то есть книжки все?..\"  \"Ну да, книжки все\". \"И от себя?\"  \"От себя\". \"От одного себя? то есть от своего имени?\"  \"Ну да, от своего имени...\" Я, кажется, очень ясно толковала, но старик очень долго не мог понять меня. \"Ну да, говорил он, задумавшись, да! это будет очень хорошо, это было бы весьма хорошо, только вы-то как же, Варвара Алексеевна?\"  \"Ну, да я ничего не подарю\". \"Как!  закричал старик, почти испугавшись, так вы ничего Петеньке не подарите, так вы ему ничего дарить не хотите?\" Старик испугался; в эту минуту он, кажется, готов был отказаться от своего предложения затем, чтобы и я могла чем-нибудь подарить его сына. Добряк был этот старик! Я уверила его, что я бы рада была подарить что-нибудь, да только у него не хочу отнимать удовольствия. \"Если сын ваш будет доволен, прибавила я, и вы будете рады, то и я буду рада, потому что втайне-то, в сердце-то моем, буду чувствовать, как будто и на самом деле я подарила\". Этим старик совершенно успокоился. Он пробыл у нас еще два часа, но всё это время на месте не мог усидеть, вставал, возился, шумел, шалил с Сашей, целовал меня украдкой, щипал меня за руку и делал тихонько гримасы Анне Федоровне. Анна Федоровна прогнала его наконец из дома. Одним словом, старик от восторга так расходился, как, может быть, никогда еще не бывало с ним. В торжественный день он явился ровно в одиннадцать часов, прямо от обедни, во фраке, прилично заштопанном, и действительно в новом жилете и в новых сапогах. В обеих руках было у него по связке книг. Мы все сидели тогда в зале у Анны Федоровны и пили кофе (было воскресенье). Старик начал, кажется, с того, что Пушкин был весьма хороший стихотворец; потом, сбиваясь и мешаясь, перешел вдруг на то, что нужно вести себя хорошо и что если человек не ведет себя хорошо, то значит, что он балуется; что дурные наклонности губят и уничтожают человека; исчислил даже несколько пагубных примеров невоздержания и заключил тем, что он с некоторого времени совершенно исправился и что теперь ведет себя примерно хорошо. Что он и прежде чувствовал справедливость сыновних наставлений, что он всё это давно чувствовал и всё на сердце слагал, но теперь и на деле стал удерживаться. В доказательство чего дарит книги на скопленные им, в продолжение долгого времени, деньги. Я не могла удержаться от слез и смеха, слушая бедного старика; ведь умел же налгать, когда нужда пришла! Книги были перенесены в комнату Покровского и поставлены на полку. Покровский тотчас угадал истину. Старика пригласили обедать. Этот день мы все были так веселы. После обеда играли в фанты, в карты; Саша резвилась, я от нее не отставала. Покровский был ко мне внимателен и всё искал случая поговорить со мною наедине, но я не давалась. Это был лучший день в целые четыре года моей жизни. А теперь всё пойдут грустные, тяжелые воспоминания; начнется повесть о моих черных днях. Вот отчего, может быть, перо мое начинает двигаться медленнее и как будто отказывается писать далее. Вот отчего, может быть, я с таким увлечением\n",
      "start_idx: [70000, 90000, 100000], chunk_sizes: [10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "# let's check (chunk_sizes are not all the same though)\n",
    "print(datasets['train'][7]['text'])\n",
    "print(f\"start_idx: {datasets['train'][7:10]['start_idx']}, chunk_sizes: {list(map(len, datasets['train'][7:10]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0b5d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597cb6ad102446f8aa893d13a4cfc4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2617 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cd8f6d3d904b3ea63f56fa62062045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266863490de24fb9872adc754c0f829d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 757\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing before chunking\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "tokenized_datasets = datasets.map(tokenize_function, remove_columns=['start_idx', 'text'], batched=True).remove_columns('attention_mask')\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4167fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_sizes: [2617, 2566, 2578, 2529, 2380, 2325, 2352, 2376, 2598, 2631]\n"
     ]
    }
   ],
   "source": [
    "# let's see what we get\n",
    "print(f\"document_sizes: {list(map(len, tokenized_datasets['train'][0:10]['input_ids']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf52baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3401c4f9c3034c70adb92172e5952dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['input_ids'],\n",
       "        num_shards: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's take these documents and chunk them up each, chunks will be of size 1024. In order to fit in RAM restrictions, we'll use IterableDataset and use generator\n",
    "from datasets import IterableDataset, IterableDatasetDict, Features, Value, Sequence\n",
    "from random import randint\n",
    "from functools import partial\n",
    "chunk_size = 1024\n",
    "\n",
    "def chunk_generator(dataset: Dataset, sequential: bool = False):\n",
    "    \"\"\"Generator that (maybe randomly) picks documents from the dataset and then random chunk of size chunk_size\"\"\"\n",
    "    if sequential:\n",
    "        for document_idx in range(len(dataset)):\n",
    "            tokens = dataset[document_idx]['input_ids']\n",
    "            i = randint(0, len(tokens) - chunk_size)\n",
    "            yield {'input_ids': tokens[i: i + chunk_size]}\n",
    "    else:\n",
    "        while True:\n",
    "            document_idx = randint(0, len(dataset) - 1)\n",
    "            tokens = dataset[document_idx]['input_ids']\n",
    "            i = randint(0, len(tokens) - chunk_size)\n",
    "            yield {'input_ids': tokens[i: i + chunk_size]}\n",
    "\n",
    "set_seed(2718)\n",
    "# it's iterable, so it's light-weight\n",
    "chunked_data = IterableDatasetDict()\n",
    "for split in tokenized_datasets:\n",
    "    dataset = tokenized_datasets[split]\n",
    "    chunked_data[split] = (IterableDataset if split == 'train' else Dataset).from_generator(\n",
    "        partial(chunk_generator, dataset=dataset, sequential=split != 'train'),\n",
    "        features=Features({'input_ids': Sequence(feature=Value(dtype='int32'))})\n",
    "    )\n",
    "tokenized_datasets = chunked_data\n",
    "chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a588ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4752ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 1252.09 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8df26f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check dtype of the dataset\n",
    "type(list(tokenized_datasets['train'].take(1))[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffbc4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) # does job for creating attention_mask and labels for us (it just copies inputs for labels, bcause shifting happens inside the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f908e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   16,  1012,   784,  ...,   669,  4223,   309],\n",
       "        [13134,  2625,   519,  ...,  1441,    16,   687],\n",
       "        [  299,  1186,  1248,  ...,   433,   968,    18],\n",
       "        [   16,   670,    16,  ...,  1335, 35338,    18],\n",
       "        [ 2754,  5804,   289,  ...,   503,  1042,    18]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[   16,  1012,   784,  ...,   669,  4223,   309],\n",
       "        [13134,  2625,   519,  ...,  1441,    16,   687],\n",
       "        [  299,  1186,  1248,  ...,   433,   968,    18],\n",
       "        [   16,   670,    16,  ...,  1335, 35338,    18],\n",
       "        [ 2754,  5804,   289,  ...,   503,  1042,    18]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = data_collator(list(tokenized_datasets[\"train\"].take(5)))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5eb39",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a4bc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ab98c",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eddbf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, # 8-256\n",
    "    lora_alpha=32, # (>= r) alpha/r ~ 2-4\n",
    "    target_modules=[\"c_attn\", \"c_proj\"], \n",
    "    lora_dropout=0.05, # 0.01-0.2\n",
    "    # bias=\"none\", # train only LoRA weights, no biases \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6099201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,488,064 || all params: 766,788,096 || trainable%: 0.8461\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11239ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c320aa84038474eb0a7860d3334e609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./rugpt3large_lora_results_long\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.5,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=1200,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./rugpt3large_lora_logs\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b684d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 9:09:00, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.242400</td>\n",
       "      <td>3.177843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.205500</td>\n",
       "      <td>3.160163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.185100</td>\n",
       "      <td>3.149015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.170100</td>\n",
       "      <td>3.141162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.141900</td>\n",
       "      <td>3.133629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.125300</td>\n",
       "      <td>3.128915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.109900</td>\n",
       "      <td>3.125694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.099100</td>\n",
       "      <td>3.121910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.083000</td>\n",
       "      <td>3.119586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.071800</td>\n",
       "      <td>3.119801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.118803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.042200</td>\n",
       "      <td>3.117959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.035500</td>\n",
       "      <td>3.117061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.024400</td>\n",
       "      <td>3.117750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.021900</td>\n",
       "      <td>3.117827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.012900</td>\n",
       "      <td>3.118428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.003900</td>\n",
       "      <td>3.118045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.001800</td>\n",
       "      <td>3.117162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.003800</td>\n",
       "      <td>3.117949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.004800</td>\n",
       "      <td>3.117759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.002600</td>\n",
       "      <td>3.118312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.003200</td>\n",
       "      <td>3.118386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.996300</td>\n",
       "      <td>3.118395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.997300</td>\n",
       "      <td>3.118546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=3.0725308787822723, metrics={'train_runtime': 32968.9864, 'train_samples_per_second': 2.329, 'train_steps_per_second': 0.036, 'total_flos': 3.239066229276672e+17, 'train_loss': 3.0725308787822723})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(11)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c871431",
   "metadata": {},
   "source": [
    "# Let's test it out (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "696898b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/illjahunovan/poetry_env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "checkpoint = 'checkpoint-1200'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=device_map, torch_dtype=\"auto\")\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0f2aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int = 40, temperature: float = 0.7, top_p: float = 0.9, seed: int | float | None = 52, **kwargs):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    set_seed(seed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,\n",
    "            **kwargs\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_text_beam(prompt: str, \n",
    "                       max_length: int = 40, \n",
    "                       num_beams: int = 10, \n",
    "                       num_return_sequences: int = 3, \n",
    "                       no_repeat_ngram_size: int = 2, \n",
    "                       num_beam_groups: int = 2,\n",
    "                       diversity_penalty: float = 1.0,\n",
    "                       seed: int | float | None = 52,\n",
    "                       **kwargs):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    set_seed(seed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequences = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams, # beam width \n",
    "            early_stopping=True, # stop when all beams finish\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size, # n-gram penalties, technique to avoid repetition\n",
    "            num_return_sequences=num_return_sequences, # return top k sequences\n",
    "            num_beam_groups=num_beam_groups, # divides num_beams to this number groups. two groups differ from each other. greatly combines with diversity_penalty \n",
    "            diversity_penalty=diversity_penalty, # encourages diversity beams (if using num_beam_groups)\n",
    "            trust_remote_code=True, # beam search backward compatibility\n",
    "            **kwargs\n",
    "        )\n",
    "    return tokenizer.batch_decode(sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e41acea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_text_beam(\"Быть или не быть?\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=1, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.0,\n",
    "                       seed=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5cbe0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Быть или не быть? Вот вопрос, на который я не мог найти никакого ответа. Я не знал, что мне делать и как мне быть. Наконец я решился и пошел к Наташе. Она сидела одна и читала книгу. Увидев меня, она тотчас же встала со стула и подошла ко мне.  Что с вами? Вы нездоровы?  спросила она, смотря на меня с каким-то беспокойством и в то же время как будто с удивлением,  я думала, вы больны.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f57a5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бог ему судья!  прибавил он, обращаясь к Алеше.  А ты, Алеша, не сердись на меня, что я так говорю,  продолжал он вдруг, как бы опомнившись, и с каким-то особенным выражением посмотрел на Алешу. Он был в чрезвычайном волнении и в то же время как будто и не в себе, но, однако ж, тотчас же взял себя в руки и продолжал как ни в чем не бывало:  Я, брат, в\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Бог ему судья! Да и не в том дело,  продолжал он, как бы опомнившись.  Я только хотел сказать, что, может быть, он и в самом деле не так уж и виноват...  А в чем же он виноват-то?  спросил я, с беспокойством смотря на него. Он не отвечал мне, но, видимо, был в сильном волнении. Вдруг он схватил меня за руку и крепко стиснул ее в своих руках. Лицо его было бледно,\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Бог ему судья!  с каким-то особенным чувством проговорил Алеша.  А я вот что думаю: может, он и в самом деле не виноват? Может, это только так кажется?  Может быть, и так,  отвечал Алешка, как бы в раздумье смотря на него. Но вдруг лицо его оживилось, губы задрожали, глаза засверкали. Он быстро подошел к Алеше, крепко обнял его и крепко поцеловал его в обе щеки:  Ну, вот видишь\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Бог ему судья!\", \n",
    "                       max_length=100, \n",
    "                       num_beams=15, \n",
    "                       num_return_sequences=15, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=3,\n",
    "                       diversity_penalty=1.0,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[5], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c35e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И все-таки, тварь ли я дрожащая или право имею?  вскричал он вдруг в исступлении.  Право имею, право!  вскричала я, вскочив с места, и бросилась к нему на шею. Он крепко прижал меня к груди и поцеловал в лоб. Я была вне себя от радости, от счастья, что он любит меня, любит так же, как и я его. Но он тотчас же отстранил меня от себя и, смотря мне прямо в глаза, сказал:\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "И все-таки, тварь ли я дрожащая или право имею? Право имею или нет? Я, может быть, и права не имею, но право-то имею!  Да, право имеете,  подтвердил Митя.  А знаете, что я вам скажу?  прибавил он вдруг, как бы вдруг опомнившись:  Знаете, я, кажется, вам не нравлюсь, потому что вы меня не любите, а я вас люблю. Я вас очень люблю, вы это знаете. Вы\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"И все-таки, тварь ли я дрожащая или право имею?\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36e76afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какое лучшее определение человека?  спросил он, смотря на меня.  Я не знаю лучшего определения человека,  отвечал я с жаром и с каким-то особенным жаром, как будто я сам был виноват в том, что он так меня мучил, и, может быть, в то же самое время я был и виноват перед ним, потому что я не понимал его, не мог понять его и не хотел понять. Он слушал меня с напряженным вниманием, но в лице его не было ни малейшего признака какого-нибудь удивления. Я продолжал:  Человек, по-моему, есть существо, созданное для того, чтобы быть счастливым,\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Какое лучшее определение человека? Я не знаю лучшего определения человека, как то, которое он сам для себя находит, и которое не может не быть самым лучшим, потому что человек есть то же самое, что и все другие существа на земле, а потому и определение его должно быть тем же самым, каким и у всех других существ, то есть как у всякого другого существа,  одним словом, это определение должно заключать в себе всю полноту и всю сущность человеческого существа. И вот это-то определение и есть самое лучшее, ибо оно не только самое точное, но и самое всеобъемлющее, самое полное и окончательное.  А что же такое это\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Какое лучшее определение человека?\", \n",
    "                       max_length=130, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39b5daff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сомневаюсь на правильном ли я пути...  пробормотал он, как бы в раздумье.  Нет, нет, не сомневайтесь!  вскричал я в восторге,  вы правы, вы совершенно правы! Вы не ошибаетесь, я не ошибся! Я именно так и думал, что вы не ошибетесь и что я именно такой человек, каким вы меня описали, и именно таким я и должен быть, потому что, если б я был другой, то я бы не мог быть таким\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Сомневаюсь на правильном ли я пути... (франц.).  Да, да, я знаю, что вы ошибаетесь,  сказал князь.  Но, может быть, это и к лучшему, потому что, если б вы знали, как я вас люблю!  прибавил он с каким-то особенным чувством, смотря на Раскольникова. Но тот не отвечал ему. Он смотрел в землю и молчал. Князь подошел к нему и взял его за руку. Тот поднял на него свой\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Сомневаюсь на правильном ли я пути...\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51805e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Русский весьма часто смеется там, где, казалось бы, должен бы плакать.  Да, это так,  подтвердил я.  Но,\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Русский весьма часто смеется там, где\", \n",
    "                       max_length=28, \n",
    "                       num_beams=15, \n",
    "                       num_return_sequences=15, \n",
    "                       no_repeat_ngram_size=3, \n",
    "                       num_beam_groups=3,\n",
    "                       diversity_penalty=1.0,\n",
    "                       seed=52)\n",
    "print(outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78016180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жизнь задыхается без вас!  вскричала она с отчаянием.  О, боже мой! Что с вами? Вы больны?  Не знаю,  отвечал он, смотря на нее с каким-то странным, как бы рассеянным видом. Он был бледен, губы его дрожали, и он не мог выговорить ни слова. Наконец он взял ее за руку и повел к себе в комнату. Она шла за ним, не зная, что с нею делается; она была в каком-\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Жизнь задыхается без воздуха, без пищи, и, может быть, уже никогда не возвратится к жизни. Я не знаю, что это такое, но я чувствую это. Мне кажется, я умираю. Но я не хочу умирать! Я хочу жить, жить и жить!  вскричал он с каким-то исступлением.  Я буду жить до тех пор, пока не умру,  продолжал он, смотря на меня в упор своими горящими глазами. Он был бледен как полотно,\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Жизнь задыхается без\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d9a1cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Главное, самому себе не лгите... ибо лгущий самому себе теряет способность говорить правду.  Ну, это уж слишком!  вскричал князь,  вы, кажется, не знаете, что такое ложь?  Нет, знаю, и очень хорошо знаю. Ложь есть ложь, а правда есть правда. Если бы вы знали, как я ненавижу ложь и как люблю правду, то, может быть, вы бы мне и не поверили. Но вы не верите, потому что не хотите верить.\n",
      "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Главное, самому себе не лгите... ибо лгущий самому себе теряет способность различать добро и зло, истину и ложь, правду и кривду, и потому не может отличить одно от другого, а потому и не в состоянии отличить добро от зла...  Да, да, я знаю, что это так!  вскричал вдруг Алеша,  я это давно знаю! Я это всегда знал и всегда буду знать, потому что я сам себе всегда лгал и никогда не буду лгать себе...\n"
     ]
    }
   ],
   "source": [
    "outputs = generate_text_beam(\"Главное, самому себе не лгите... ибо лгущий самому себе теряет способность\", \n",
    "                       max_length=100, \n",
    "                       num_beams=10, \n",
    "                       num_return_sequences=10, \n",
    "                       no_repeat_ngram_size=2, \n",
    "                       num_beam_groups=2,\n",
    "                       diversity_penalty=1.5,\n",
    "                       seed=52)\n",
    "print(outputs[0], outputs[-1], sep='\\n' + '_' * 1000 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc0a31",
   "metadata": {},
   "source": [
    "## Some conclusions after experimenting\n",
    "- LoRA algorithm is amazing for fine-tuning large and medium size llms \\\n",
    "it provides a way to train even when other common memory saving techniques couldn't help out\n",
    "- adafactor optimizer (though it wasn't used eventually) is a fantastic replacement for AdamW\n",
    "- gradient_accumulation idea is a real saver for imitating large batch size\n",
    "- quantinization and fp16 especially are musthave for nlp tasks\n",
    "- iterable, streaming datasets combining with generators are light-weight and really handy   \n",
    "- DataCollatorForLanguageModeling, return_overflowing_tokens, return_overflowing_tokens and other stuff from hf libraries rid of so much boilerplate code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10 Poetry",
   "language": "python",
   "name": "poetry_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
